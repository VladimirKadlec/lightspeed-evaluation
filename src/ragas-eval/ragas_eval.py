"""Evaluation of LightSpeed services using Ragas"""

import logging
import re
import sys

import click
import dotenv
import pandas as pd
import ragas
from langchain_openai import ChatOpenAI
from ragas import EvaluationDataset, SingleTurnSample
from ragas.cache import DiskCacheBackend
from ragas.llms import LangchainLLMWrapper
from ragas.metrics import ResponseRelevancy

dotenv.load_dotenv()

logger = logging.getLogger(__name__)


def read_qna(filename: str) -> pd.DataFrame:
    """Read question answer pairs from json"""
    logger.info("Reading input qnas from %s", filename)
    return pd.read_json(filename)


def evaluate_model(
    df: pd.DataFrame, model_column: str, metrics: ragas.metrics.Metric
) -> ragas.dataset_schema.EvaluationResult:
    """Evaluate answers from model_column"""
    samples = []
    for i in range(len(df)):
        row = df.iloc[i]
        samples.append(
            SingleTurnSample(user_input=row["question"], response=row[model_column])
        )

    return ragas.evaluate(dataset=EvaluationDataset(samples), metrics=metrics)


@click.command(
    help="""
Get annotations from Label Studio projects
""",
    #    no_args_is_help=True,
    context_settings={
        "help_option_names": ["-h", "--help"],
        "show_default": True,
    },
)
@click.option(
    "-v",
    "--verbose",
    default=False,
    is_flag=True,
    help="Increase the logging level to DEBUG",
)
@click.option(
    "-i",
    "--input-filename",
    default="../../eval_output/generated_qna.json",
    type=click.Path(exists=True),
    help="""
Input JSON file with question/answer pairs -- DataFrame in JSON generated by lightspeed-evaluation/generate_answers
""",
)
def main(verbose: bool, input_filename: str) -> int:
    """Run main entrypoint."""
    logging_level = logging.INFO
    if verbose:
        logging_level = logging.DEBUG
    logging.basicConfig(
        datefmt="%Y-%m-%d %H:%M:%S",
        format="%(asctime)s [%(name)s:%(filename)s:%(lineno)d] %(levelname)s: %(message)s",
    )
    logger.setLevel(logging_level)

    df = read_qna(input_filename)

    # TODO cache embeddings model as well
    ragas_cacher = DiskCacheBackend(cache_dir="ragas_llm_cache")
    cached_llm = LangchainLLMWrapper(ChatOpenAI(model="gpt-4o"), cache=ragas_cacher)

    metrics = [ResponseRelevancy(llm=cached_llm)]

    pattern = re.compile("^.*_answers$")
    answer_cols = [c for c in list(df.columns) if pattern.match(c)]
    for model in answer_cols:
        print(f"====={model}=====")

        res = evaluate_model(df, model, metrics)
        print(res)


if __name__ == "__main__":
    sys.exit(main())  # pylint: disable=E1120
