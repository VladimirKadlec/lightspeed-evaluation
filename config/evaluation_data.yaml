# LightSpeed Evaluation Framework - Sample/Mock Data

- conversation_group_id: "conv_group_1"
  description: "conversation group description"

  conversation_metrics: []
  conversation_metrics_metadata: {}

  turns:
    - turn_id: "1"
      query: "User query"
      response: "API response"
      contexts:
        - "Context 1"
        - "Context 2"
      expected_response: "Expected Response"

      turn_metrics:
        - "ragas:faithfulness"
        - "ragas:response_relevancy"
        - "ragas:context_precision_without_reference"

      turn_metrics_metadata:
        "ragas:faithfulness":
          threshold: 0.99

- conversation_group_id: "conv_group_2"
  description: "conversation group description"
  conversation_metrics: []
  conversation_metrics_metadata: {}

  turns:
    - turn_id: "1"
      query: "User Query"
      response: "API Response"
      contexts:
        - "Context 1"
      expected_response: "Expected Response"

      turn_metrics:
        - "ragas:context_recall"
        - "ragas:context_relevance"
        - "ragas:context_precision_with_reference"

- conversation_group_id: "conv_group_3"
  description: "conversation group description"

  conversation_metrics:
    - "deepeval:conversation_completeness"
    - "deepeval:conversation_relevancy"

  conversation_metrics_metadata: {}

  turns:
    - turn_id: "1"
      query: "User Query 1"

      turn_metrics: []  # Skip eval for this turn

    - turn_id: "2"
      query: "User Query 2"
      response: "API Response 2"
      # Use default metric from system config (Ex: response_relevancy)
