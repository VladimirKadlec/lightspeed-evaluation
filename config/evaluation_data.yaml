# LightSpeed Evaluation Framework - Sample/Mock Data

- conversation_group_id: conv_group_1
  description: conversation group description

  conversation_metrics: []
  conversation_metrics_metadata: {}
  turns:
    - turn_id: turn_id1
      query: User query
      attachments: []
      response: API response
      tool_calls: null
      contexts:
        - Context 1
        - Context 2
      expected_response: Expected Response
      expected_tool_calls: null
      turn_metrics:
      - ragas:faithfulness
      - ragas:response_relevancy
      - ragas:context_precision_without_reference
      turn_metrics_metadata:
        ragas:faithfulness:
          threshold: 0.99
      verify_script: null
  setup_script: null
  cleanup_script: null

- conversation_group_id: conv_group_2
  description: conversation group description
  conversation_metrics: []
  conversation_metrics_metadata: {}
  turns:
    - turn_id: turn_id1
      query: User Query
      contexts:
        - Context 1
      expected_response: Expected Response
      turn_metrics:
        - ragas:context_recall
        - ragas:context_relevance
        - ragas:context_precision_with_reference

- conversation_group_id: conv_group_3
  description: conversation group description
  conversation_metrics:
    - deepeval:conversation_completeness
    - deepeval:conversation_relevancy
  turns:
    - turn_id: turn_id1  # Skip evaluation
      query: User Query 1
      turn_metrics: []
    - turn_id: turn_id2  # Default evaluation
      query: User Query 2

- conversation_group_id: script_eval_example
  description: Example of script-based evaluation for namespace testing
  setup_script: sample_scripts/setup.sh
  cleanup_script: sample_scripts/cleanup.sh

  turns:
    - turn_id: turn_id1
      query: Create a namespace called ols-test-ns
      turn_metrics:
        - script:action_eval
      verify_script: sample_scripts/verify.sh
